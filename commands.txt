hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.0.jar wordcount /spark/logs /spark/logs_wordcount_hadoop
47.38secs
====================================================================================================================================================
MASTER=spark://nimbus-head.cs.wright.edu:7077 ./bin/spark-shell
MASTER=spark://nimbus-head.cs.wright.edu:7077 ./bin/pyspark
====================================================================================================================================================
Scala:
====================================================================================================================================================
val txtfile=sc.textFile("hdfs:///spark/logs")
val counts = txtfile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _)
counts.saveAsTextFile("hdfs:///spark/logs_wordcount")
6.37secs

val txtfile=sc.textFile("hdfs:///spark/logs")
val startup = txtfile.filter(line => line.contains("STARTUP_MSG"))
txtfile.cache()
startup.count()
time:0.26

val txtfile1=sc.textFile("hdfs:///spark/logs")
val startup1 = txtfile1.filter(line => line.contains("STARTUP_MSG"))
startup1.count()
time:1.68sec
====================================================================================================================================================
Python:
====================================================================================================================================================
pets = sc.parallelize([("cat", 1), ("dog", 1), ("cat", 2)])
pets.reduceByKey(lambda x, y: x + y)
pets.groupByKey()
pets.sortByKey()

visits = sc.parallelize([("index.html", "1.2.3.4"),("about.html", "3.4.5.6"),("index.html", "1.3.3.1")])
pageNames = sc.parallelize([("index.html", "Home"),("about.html", "About")])
visits.join(pageNames)

counts = txtfile.flatMap(lambda line:line.split(" ")).map(lambda word : (word, 1)).reduceByKey(lambda x, y: x + y)
txtfile=sc.textFile("hdfs:///spark/logs")
counts.collect()[0]

words = txtfile.flatMap(lambda line:line.split(" ")).map(lambda word : (word, 1))
words.reduceByKey(lambda x, y: x + y, 5)

